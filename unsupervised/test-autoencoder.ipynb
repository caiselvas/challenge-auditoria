{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (233, 5)\n",
      "Shape of validation data: (59, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "dataorig = pd.read_excel(\"C:/Users/Usuario/Documents/Projectes/ChallengeAuditoria/challenge-auditoria/data/inventory_data_new.xlsx\")\n",
    "\n",
    "dataorig[\"cost_valor\"] = dataorig[\"preu_venda_unitari_2023\"] / dataorig[\"cost_unitari_stock_2023\"]\n",
    "# Drop rows with missing values\n",
    "\n",
    "data = dataorig.copy()\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Encode categorical variables if needed\n",
    "# (If there are any categorical variables that need encoding)\n",
    "\n",
    "# Select relevant features for the Autoencoder\n",
    "features = [ 'proporcio_variacio_preu_venda_unitari_2022_2023', 'diferencia_entrada_sortida', 'dies_ultima_sortida', 'stock_final_2023', 'cost_valor']\n",
    "\n",
    "X = data[features]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training data:\", X_train.shape)\n",
    "print(\"Shape of validation data:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 1.0524 - val_loss: 0.7411\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.9676 - val_loss: 0.7156\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2190 - val_loss: 0.6940\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8814 - val_loss: 0.6754\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.0822 - val_loss: 0.6585\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.7602 - val_loss: 0.6452\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7052 - val_loss: 0.6322\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.0117 - val_loss: 0.6181\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8866 - val_loss: 0.6042\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8508 - val_loss: 0.5914\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7669 - val_loss: 0.5807\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7505 - val_loss: 0.5716\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6849 - val_loss: 0.5611\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7777 - val_loss: 0.5529\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7014 - val_loss: 0.5423\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7770 - val_loss: 0.5352\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7284 - val_loss: 0.5320\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8062 - val_loss: 0.5302\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6580 - val_loss: 0.5280\n",
      "Epoch 20/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7729 - val_loss: 0.5274\n",
      "Epoch 21/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8427 - val_loss: 0.5259\n",
      "Epoch 22/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6366 - val_loss: 0.5246\n",
      "Epoch 23/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7978 - val_loss: 0.5217\n",
      "Epoch 24/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6066 - val_loss: 0.5184\n",
      "Epoch 25/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7336 - val_loss: 0.5117\n",
      "Epoch 26/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6902 - val_loss: 0.5007\n",
      "Epoch 27/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5344 - val_loss: 0.4919\n",
      "Epoch 28/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6919 - val_loss: 0.4826\n",
      "Epoch 29/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5621 - val_loss: 0.4748\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5241 - val_loss: 0.4658\n",
      "Epoch 31/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4759 - val_loss: 0.4562\n",
      "Epoch 32/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4611 - val_loss: 0.4436\n",
      "Epoch 33/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5364 - val_loss: 0.4321\n",
      "Epoch 34/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4481 - val_loss: 0.4221\n",
      "Epoch 35/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5069 - val_loss: 0.4134\n",
      "Epoch 36/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5042 - val_loss: 0.4058\n",
      "Epoch 37/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5003 - val_loss: 0.4011\n",
      "Epoch 38/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5135 - val_loss: 0.3984\n",
      "Epoch 39/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4109 - val_loss: 0.3956\n",
      "Epoch 40/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4020 - val_loss: 0.3956\n",
      "Epoch 41/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3553 - val_loss: 0.3941\n",
      "Epoch 42/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4191 - val_loss: 0.3929\n",
      "Epoch 43/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3837 - val_loss: 0.3922\n",
      "Epoch 44/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4064 - val_loss: 0.3908\n",
      "Epoch 45/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5144 - val_loss: 0.3906\n",
      "Epoch 46/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4107 - val_loss: 0.3900\n",
      "Epoch 47/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3820 - val_loss: 0.3898\n",
      "Epoch 48/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3389 - val_loss: 0.3895\n",
      "Epoch 49/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3487 - val_loss: 0.3887\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3522 - val_loss: 0.3883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1508478db90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define Autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 4  # Increase the size of the encoding layer\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(32, activation='relu')(input_layer)  # Add more neurons\n",
    "encoder = Dense(encoding_dim, activation='relu')(encoder)\n",
    "decoder = Dense(32, activation='relu')(encoder)  # Add more neurons\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the Autoencoder model\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Assets needing depreciation:\n",
      "       material  unitats_2022  vendes_2022  preu_venda_unitari_2022  \\\n",
      "90  133066450.0      504000.0     17266.32                 0.034259   \n",
      "\n",
      "    unitats_2023  vendes_2023  preu_venda_unitari_2023  \\\n",
      "90     1135800.0     35866.95                 0.031579   \n",
      "\n",
      "    variacio_preu_venda_unitari_2022_2023  \\\n",
      "90                               -0.00268   \n",
      "\n",
      "    proporcio_variacio_preu_venda_unitari_2022_2023 data_darrera_entrada  \\\n",
      "90                                        -0.078228           2023-09-27   \n",
      "\n",
      "    dies_ultima_entrada data_darrera_sortida  dies_ultima_sortida  \\\n",
      "90                 95.0           2023-10-19                 73.0   \n",
      "\n",
      "    diferencia_entrada_sortida  stock_final_2023  valor_total_stock_2023  \\\n",
      "90                        22.0            3600.0                   47.66   \n",
      "\n",
      "    cost_unitari_stock_2023  cost_valor  \n",
      "90                 0.013239    2.385289  \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming reconstructions and X_val are numpy arrays\n",
    "reconstructions = autoencoder.predict(X_val)\n",
    "mse = tf.keras.losses.mean_squared_error(X_val, reconstructions)\n",
    "\n",
    "# Calculate reconstruction errors (no need to specify axis for 1D array)\n",
    "mse_numpy = mse.numpy()\n",
    "reconstruction_errors = mse_numpy\n",
    "\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = reconstruction_errors.mean() + 2 * reconstruction_errors.std()\n",
    "\n",
    "# Identify assets with reconstruction errors above the threshold\n",
    "anomalies_indices = [i for i, error in enumerate(reconstruction_errors) if error > threshold]\n",
    "anomalies = data.iloc[anomalies_indices]\n",
    "\n",
    "print(\"Assets needing depreciation:\")\n",
    "print(anomalies)\n",
    "print(len(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets needing depreciation beyond acceptable range:\n",
      "        material  unitats_2022  vendes_2022  preu_venda_unitari_2022  \\\n",
      "10   115030252.0       75900.0      6825.62                 0.089929   \n",
      "100  133066532.0           NaN          NaN                      NaN   \n",
      "124  133100502.0      240000.0     13191.12                 0.054963   \n",
      "453  152270347.0           NaN          NaN                      NaN   \n",
      "569  153010114.0      723800.0     24060.01                 0.033241   \n",
      "570  153010115.0     7937600.0    320326.09                 0.040356   \n",
      "607  153110150.0    35825400.0    812679.69                 0.022684   \n",
      "608  153110151.0     2906800.0     63774.72                 0.021940   \n",
      "612  153110155.0     2756000.0     58779.76                 0.021328   \n",
      "614  153110157.0     7384000.0    168335.44                 0.022797   \n",
      "962  173430205.0    11835000.0    413846.10                 0.034968   \n",
      "982  182040131.0      332800.0      8366.59                 0.025140   \n",
      "\n",
      "     unitats_2023  vendes_2023  preu_venda_unitari_2023  \\\n",
      "10        27600.0      2127.68                 0.077090   \n",
      "100       36000.0      1041.48                 0.028930   \n",
      "124       60000.0      3084.00                 0.051400   \n",
      "453     4960000.0     63988.00                 0.012901   \n",
      "569      745800.0     22081.00                 0.029607   \n",
      "570     4294400.0    132208.62                 0.030786   \n",
      "607    34418800.0    669179.16                 0.019442   \n",
      "608     1612000.0     29900.00                 0.018548   \n",
      "612      988000.0     18048.16                 0.018267   \n",
      "614     3744000.0     71062.16                 0.018980   \n",
      "962     5378400.0    204323.56                 0.037990   \n",
      "982      524800.0     12416.89                 0.023660   \n",
      "\n",
      "     variacio_preu_venda_unitari_2022_2023  \\\n",
      "10                               -0.012839   \n",
      "100                                    NaN   \n",
      "124                              -0.003563   \n",
      "453                                    NaN   \n",
      "569                              -0.003634   \n",
      "570                              -0.009569   \n",
      "607                              -0.003242   \n",
      "608                              -0.003391   \n",
      "612                              -0.003061   \n",
      "614                              -0.003817   \n",
      "962                               0.003022   \n",
      "982                              -0.001480   \n",
      "\n",
      "     proporcio_variacio_preu_venda_unitari_2022_2023 data_darrera_entrada  \\\n",
      "10                                         -0.142771           2024-01-24   \n",
      "100                                              NaN           2023-12-12   \n",
      "124                                        -0.064825           2023-03-16   \n",
      "453                                              NaN           2024-03-01   \n",
      "569                                        -0.109325           2024-02-22   \n",
      "570                                        -0.237124           2024-02-22   \n",
      "607                                        -0.142926           2024-03-04   \n",
      "608                                        -0.154580           2024-02-28   \n",
      "612                                        -0.143500           2024-02-01   \n",
      "614                                        -0.167434           2024-02-27   \n",
      "962                                         0.086412           2023-11-28   \n",
      "982                                        -0.058861           2023-10-05   \n",
      "\n",
      "     dies_ultima_entrada data_darrera_sortida  dies_ultima_sortida  \\\n",
      "10                 -24.0           2023-12-05                 26.0   \n",
      "100                 19.0           2024-01-18                -18.0   \n",
      "124                290.0           2023-08-30                123.0   \n",
      "453                -61.0           2024-02-27                -58.0   \n",
      "569                -53.0           2024-02-16                -47.0   \n",
      "570                -53.0           2024-02-26                -57.0   \n",
      "607                -64.0           2024-03-01                -61.0   \n",
      "608                -59.0           2024-02-02                -33.0   \n",
      "612                -32.0           2024-02-02                -33.0   \n",
      "614                -58.0           2024-03-01                -61.0   \n",
      "962                 33.0           2023-11-08                 53.0   \n",
      "982                 87.0           2023-12-18                 13.0   \n",
      "\n",
      "     diferencia_entrada_sortida  stock_final_2023  valor_total_stock_2023  \\\n",
      "10                        -50.0           23000.0                 1777.15   \n",
      "100                        37.0           36000.0                 1096.72   \n",
      "124                       167.0           60000.0                 3114.45   \n",
      "453                        -3.0          400000.0                 5588.00   \n",
      "569                        -6.0           88000.0                 2722.72   \n",
      "570                         4.0          264000.0                 8168.16   \n",
      "607                        -3.0         2904200.0                58868.13   \n",
      "608                       -26.0          104000.0                 2108.08   \n",
      "612                         1.0           80600.0                 1633.76   \n",
      "614                         3.0          520000.0                10540.40   \n",
      "962                       -20.0          810000.0                32353.43   \n",
      "982                        74.0           25600.0                  655.62   \n",
      "\n",
      "     cost_unitari_stock_2023  cost_valor  \n",
      "10                  0.077267    0.997702  \n",
      "100                 0.030464    0.949632  \n",
      "124                 0.051908    0.990223  \n",
      "453                 0.013970    0.923465  \n",
      "569                 0.030940    0.956921  \n",
      "570                 0.030940    0.995032  \n",
      "607                 0.020270    0.959164  \n",
      "608                 0.020270    0.915066  \n",
      "612                 0.020270    0.901203  \n",
      "614                 0.020270    0.936373  \n",
      "962                 0.039943    0.951108  \n",
      "982                 0.025610    0.923861  \n"
     ]
    }
   ],
   "source": [
    "# Define accounting standards\n",
    "salvage_value = 0  # Assumption: Salvage value is 0 (can be adjusted)\n",
    "original_cost = dataorig['cost_unitari_stock_2023']  # Original cost of the asset\n",
    "\n",
    "# Calculate the depreciation expense using the straight-line method\n",
    "# We need to estimate the useful life or make an assumption\n",
    "# For demonstration, let's assume a useful life of 5 years\n",
    "useful_life_years = 5\n",
    "depreciation_expense = (original_cost - salvage_value) / useful_life_years\n",
    "\n",
    "# Define an acceptable range for depreciation value\n",
    "# We can use a percentage of the original cost or a fixed value\n",
    "# For demonstration, let's use a percentage (e.g., 10%) of the original cost\n",
    "acceptable_range_percentage = 0  # 10%\n",
    "acceptable_depreciation_range = original_cost * acceptable_range_percentage\n",
    "\n",
    "# Calculate the lower and upper bounds of the acceptable range\n",
    "lower_bound = original_cost - acceptable_depreciation_range\n",
    "upper_bound = original_cost + acceptable_depreciation_range\n",
    "\n",
    "# Determine assets that fall outside the acceptable range\n",
    "outside_acceptable_range = (dataorig['preu_venda_unitari_2023'] < lower_bound)\n",
    "assets_outside_range = dataorig[outside_acceptable_range]\n",
    "\n",
    "print(\"Assets needing depreciation beyond acceptable range:\")\n",
    "print(assets_outside_range)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
